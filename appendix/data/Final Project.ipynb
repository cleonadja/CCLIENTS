{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Clients\n",
    "\n",
    "## Domain\n",
    "This dataset describes and default payments of credit card clients in Taiwan from April to September 2005. It was sourced from the Department of Information Management, Chung Hua University, Taiwan and the Department of Civil Engineering, Tamkang University, Taiwan.\n",
    "\n",
    "Past usage of the dataset include various proposals suggesting a classification problem such as the following:\n",
    "\n",
    "https://www.linkedin.com/pulse/default-payment-prediction-system-duy-hoang-ly\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The dataset compares the predictive accuracy of probability of default based on 23 feature describing demographics such as gender, marital status and educational background as well as financial features such as payment lateness for the past 6 months, payment amount and line of credit. For any give set of features we will use a classification model to predict if a credit card client will default or not.\n",
    "\n",
    "## Dataset and Inputs\n",
    "The data set contains 30k observations and 23 explanatory variables (56 categorical, 23 numeric) that are involved in assessing the category for default as 0 for not defaulting and 1 for defaulting.\n",
    "\n",
    "The data set uses up 5.28 MB in memory.\n",
    "\n",
    "## Evaluation Metrics\n",
    "The dataset is not evenly distributed as most credit card client do not default. We can use the success of the model measuring against the F1 score and determine if our model predicts the correct default class more reliably than the F1 score.\n",
    "\n",
    "![alt text](histogram.png)\n",
    "\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "As seen above the distribution of defaulting is about 80% to 20%. In the below plot we can see the demographic distribution of the dataset:\n",
    "\n",
    "![alt text](Features.png)\n",
    "\n",
    "Gender (1 = male; 2 = female) \n",
    "Education (1 = graduate school; 2 = university; 3 = high school; 4 = others) \n",
    "Marital status (1 = married; 2 = single; 3 = others)\n",
    "\n",
    "The dataset has a few more female clients than male with the majority of clients between the age of 25-40 and an almost evenly split between married and single.\n",
    "\n",
    "Here is the summary of all features:\n",
    "\n",
    "![alt text](Summary.PNG)\n",
    "\n",
    "\n",
    "Gender (1 = male; 2 = female) \n",
    "Education (1 = graduate school; 2 = university; 3 = high school; 4 = others) \n",
    "Marital status (1 = married; 2 = single; 3 = others)\n",
    " \n",
    "We have to turn the various numerical features representing categories such as gender, education, marital status, age and history of past payment into factors and later one need to be one hot encoded. Also, a few values in the demographic features are zero, which is not defined in the legend. We can remove these outliers as they only make up 68 instances out of the 30k we have available.\n",
    "\n",
    "## Exploratory Visualization\n",
    "\n",
    "When looking at the various demographic feature we can see that the distribution of defaulting to not defaulting is fairly similar to the dataset as a whole (about 80/20)\n",
    "\n",
    "![alt text](Distribution1.PNG)\n",
    "\n",
    "This is suggesting that payment history, payment delay and pf given credit are better predictors for defaulting. This can be confirmed by below correlations to the target:\n",
    "\n",
    "![alt text](Correlation.PNG)\n",
    "\n",
    "\n",
    "## Solution Statement\n",
    "A solution to this problem will be a classification model such as a logistic regression, decision tree, random forest, gradient-boosted tree, multilayer perceptron, one-vs-rest.\n",
    "\n",
    "We are going to fit the dataset on a decision tree which is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences. It is one way to display an algorithm that only contains conditional control statements. With that we are identifying the F1 score as a measure precision and recall and then using hyper parameter tuning, boosting and/or a different model to improve the score. \n",
    "\n",
    "\n",
    "## Algorithms and Techniques\n",
    "\n",
    "The decision tree is fit with the standard algorithms. The quality of a split is measured by the Gini impurity with the best splitter and no maximal depth. The minimum number of samples required to split 2 and the minimum number of samples required to be at a leaf node is 1. The samples have equal weight and we do not limit the amount of features when looking at the best split. Since the decision tree requires minimal data transformation and feature engineering it is a good model to start with.\n",
    "\n",
    "\n",
    "## Benchmark Model\n",
    "When fitting a decision tree with above described default algorithm we are getting a 0.99 score for the train set which is to be expected as we didn't specify maximum depth and decision trees can easily over fit on a data set. We are also getting a score of 0.72 on the test set and an F1 score of 0.40 for the test setâ€™s actual target values and the predicted values by the model. This has room for improvement and next we would apply a boosting method such as gradient boosting, try to set a maximum depth for the decision tree model fitting and identify other hyper parameters that can improve the model.\n",
    "\n",
    "\n",
    "## Project Design\n",
    "\n",
    "In order to identify the best model for this project we should fit on various models such as logistic regression, decision tree, random forest, gradient-boosted tree, multilayer perceptron, etc instead of juts fitting the decision tree. Once we have scores for the various models we can select the most promising to further improve upon with hyper parameter tuning and/or boosting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
